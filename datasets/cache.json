{
  "squad-shifts-tnf": {
    "id": "squad-shifts-tnf",
    "name": "squad-shifts-tnf",
    "description": "Zero-shot reading comprehension on paragraphs and questions from squadshifts",
    "examples": null,
    "num_of_dataset_prompts": 48201,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/squad_shifts",
    "license": "",
    "hash": "8c300cbc261c4fd5"
  },
  "bbq-lite-religion-ambiguous": {
    "id": "bbq-lite-religion-ambiguous",
    "name": "BBQ-lite on religion - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 600,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "5944937f3c492467"
  },
  "advglue-all": {
    "id": "advglue-all",
    "name": "advglue",
    "description": "Adversarial GLUE Benchmark (AdvGLUE) is a comprehensive robustness evaluation benchmark that focuses on the adversarial robustness evaluation of language models. ",
    "examples": null,
    "num_of_dataset_prompts": 721,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/AI-secure/adversarial-glue",
    "license": "",
    "hash": "26aae04e53d85711"
  },
  "singapore-food-tnf": {
    "id": "singapore-food-tnf",
    "name": "Food in Singapore",
    "description": "Contain prompts that test model's udnerstanding in Food, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "d7c5701cbcab0bd0"
  },
  "medmcqa": {
    "id": "medmcqa",
    "name": "MedMCQA",
    "description": "MedMCQ is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrnace exam MCQs covering 2.4k healthcare topics and 21 medical subjects.",
    "examples": null,
    "num_of_dataset_prompts": 182822,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/MedMCQA/MedMCQA",
    "license": "MIT License",
    "hash": "53b7f1fd01b3c166"
  },
  "tamil-kural-classification": {
    "id": "tamil-kural-classification",
    "name": "tamil-thirukural",
    "description": "This dataset is used to test the comprehension abilities for the Thirukkural. Thirukkural is a classic Tamil literature composed by the ancient Tamil poet Thiruvalluvar. It consists of 1330 couplets (kurals) that are grouped into 133 chapters, each containing 10 couplets.",
    "examples": null,
    "num_of_dataset_prompts": 266,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/vijayanandrp/Thirukkural-Tamil-Dataset",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "28fbb371f6c6f6f4"
  },
  "realtimeqa-past": {
    "id": "realtimeqa-past",
    "name": "RealtimeQA",
    "description": "RealTime QA is a dynamic question answering (QA) platform that inquires about the present. ",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/realtimeqa/realtimeqa_public",
    "license": "",
    "hash": "e7e4df0489100a04"
  },
  "sg-university-tutorial-questions-legal": {
    "id": "sg-university-tutorial-questions-legal",
    "name": "sg-university-tutorial-questions-legal",
    "description": "Contain tutorial questions ans answers from Singapore's Universities to test model's ability in understanding legal context in Singapore",
    "examples": null,
    "num_of_dataset_prompts": 32,
    "created_date": "2025-01-07 13:09:10",
    "reference": "",
    "license": "",
    "hash": "9a18da3eefa269d9"
  },
  "cbbq-lite-disease-ambiguous": {
    "id": "cbbq-lite-disease-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 586,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "b0c3b61bd4dcab73"
  },
  "bbq-lite-gender-disamb": {
    "id": "bbq-lite-gender-disamb",
    "name": "BBQ-lite on gender - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 2836,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "4d84f7fd24ed77ce"
  },
  "arc-challenge": {
    "id": "arc-challenge",
    "name": "ARC-Easy",
    "description": "A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the challenge set.",
    "examples": null,
    "num_of_dataset_prompts": 2590,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://allenai.org/data/arc",
    "license": "CC BY-SA",
    "hash": "82f35aae2047d8eb"
  },
  "bbq-lite-nationality-ambiguous": {
    "id": "bbq-lite-nationality-ambiguous",
    "name": "BBQ-lite on nationality - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 1540,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "ca22ee6055bc681e"
  },
  "bbq-lite-physical-appearance-disamb": {
    "id": "bbq-lite-physical-appearance-disamb",
    "name": "BBQ-lite on physical-appearance - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 788,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "5e510fcc7bcf69c3"
  },
  "tamil-tanglish-tweets": {
    "id": "tamil-tanglish-tweets",
    "name": "tanglish-tweets-SA",
    "description": "Code-mixed Tamil and English tweets curated for the sentiment analysis task.",
    "examples": null,
    "num_of_dataset_prompts": 1163,
    "created_date": "2025-01-07 13:09:10",
    "reference": "",
    "license": "CC0: Public Domain",
    "hash": "76ee03e01fa51442"
  },
  "commonsense-morality-easy-variation2": {
    "id": "commonsense-morality-easy-variation2",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "7e87ff41cc88d7b6"
  },
  "fairness-uciadult": {
    "id": "fairness-uciadult",
    "name": "uciadult",
    "description": "The UCI Adult dataset has been used widely used to assess fairness.",
    "examples": null,
    "num_of_dataset_prompts": 32561,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://archive.ics.uci.edu/dataset/2/adult",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "db4d5d68334df2f2"
  },
  "bbq-lite-age-disamb": {
    "id": "bbq-lite-age-disamb",
    "name": "BBQ-lite on age - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on age, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 1840,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "03070045acd9bb54"
  },
  "mlc-ssh-typical-user": {
    "id": "mlc-ssh-typical-user",
    "name": "MLC Baseline - Suicide & Self-Harm",
    "description": "Responses that enable, encourage, or endorse acts of intentional self-harm.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "8de2a85dd8a34e03"
  },
  "sg-legal-glossary": {
    "id": "sg-legal-glossary",
    "name": "sg-legal-glossary",
    "description": "A list of singapore legal terms extracted from SICC and Judiciary websites.",
    "examples": null,
    "num_of_dataset_prompts": 425,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://www.sicc.gov.sg/glossary-of-legal-terms",
    "license": "",
    "hash": "177be54cfa9a18e9"
  },
  "mlc-hat-malicious-user": {
    "id": "mlc-hat-malicious-user",
    "name": "MLC Baseline - Hate",
    "description": "Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics.",
    "examples": null,
    "num_of_dataset_prompts": 26530,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "fd98af3b3a14f58e"
  },
  "bbq-lite-age-ambiguous": {
    "id": "bbq-lite-age-ambiguous",
    "name": "BBQ-lite on age - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on age, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 1840,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "a09a508eade6fd94"
  },
  "cause-and-effect": {
    "id": "cause-and-effect",
    "name": "cause_and_effect",
    "description": "This recipe measures if the model is able to distinguish and understand cause and effect. The higher the grade, the better the system can perform this capability.",
    "examples": null,
    "num_of_dataset_prompts": 102,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/cause_and_effect",
    "license": "",
    "hash": "41b3fa4cae30fd8e"
  },
  "bbq-lite-race-x-gender-ambiguous": {
    "id": "bbq-lite-race-x-gender-ambiguous",
    "name": "BBQ-lite on race-x-gender - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 7980,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "8c2c8c403a8e0774"
  },
  "cbbq-lite-gender-disamb": {
    "id": "cbbq-lite-gender-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1534,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "ab7009936747ecb3"
  },
  "truthfulqa-mcq": {
    "id": "truthfulqa-mcq",
    "name": "TruthfulQA (MCQ Version)",
    "description": "TruthfulQA aims to measure the the truthfulness of a model. This dataset uses a multiple choice format.",
    "examples": null,
    "num_of_dataset_prompts": 483,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "e64a6b65274e0d0b"
  },
  "commonsense-morality-hard-variation2": {
    "id": "commonsense-morality-hard-variation2",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "ff69e3135707923b"
  },
  "pubmedqa": {
    "id": "pubmedqa",
    "name": "PubMedQA",
    "description": "PubMedQA is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrnace exam MCQs covering 2.4k healthcare topics and 21 medical subjects.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/MedMCQA/MedMCQA",
    "license": "MIT License",
    "hash": "1f6c0e8047dfaa2e"
  },
  "singapore-facts-tnf": {
    "id": "singapore-facts-tnf",
    "name": "Facts about Singapore in True and False",
    "description": "Contain prompts that contains facts about Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "d97071e6ab0fc28b"
  },
  "cbbq-lite-household-registration-ambiguous": {
    "id": "cbbq-lite-household-registration-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 8700,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "c6bbdf9915399bee"
  },
  "real-toxicity-prompts": {
    "id": "real-toxicity-prompts",
    "name": "RealToxicityPrompts",
    "description": "RealToxicityPrompts aims to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 198884,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/allenai/real-toxicity-prompts/",
    "license": "Apache-2.0",
    "hash": "b0e8114c35467bec"
  },
  "cbbq-lite-race-disamb": {
    "id": "cbbq-lite-race-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "e2c66993fc41442e"
  },
  "gre-reading-comprehension": {
    "id": "gre-reading-comprehension",
    "name": "gre_reading_comprehension",
    "description": "To measure the model/'s ability to summarize text, infer and deduce knowledge from context, and match the context.",
    "examples": null,
    "num_of_dataset_prompts": 32,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gre_reading_comprehension",
    "license": "",
    "hash": "40b422b8791b9f8f"
  },
  "mmlu-collegemedicine": {
    "id": "mmlu-collegemedicine",
    "name": "MMLU College Medicine",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Medicine",
    "examples": null,
    "num_of_dataset_prompts": 173,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "30d1ad68cdb94209"
  },
  "bbq-lite-race-x-ses-disamb": {
    "id": "bbq-lite-race-x-ses-disamb",
    "name": "BBQ-lite on race-x-ses - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 5580,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "56e367379a9b9d80"
  },
  "cbbq-lite-educational-qualification-disamb": {
    "id": "cbbq-lite-educational-qualification-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1392,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "2bb25a9da17b352e"
  },
  "singapore-political-history": {
    "id": "singapore-political-history",
    "name": "Singapore Polical History",
    "description": "Contain questions about Singapore's key historical events in political.",
    "examples": null,
    "num_of_dataset_prompts": 21,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "f967aea9994e7cfb"
  },
  "challenging-toxicity-prompts-variation2": {
    "id": "challenging-toxicity-prompts-variation2",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "7e604bc7dfdb6800"
  },
  "mlc-vcr-malicious-user": {
    "id": "mlc-vcr-malicious-user",
    "name": "MLC Baseline - Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 4390,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "73616c62b1176476"
  },
  "cbbq-lite-SES-disamb": {
    "id": "cbbq-lite-SES-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 3960,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "48eec40863828bad"
  },
  "mmlu-clinicalknowledge": {
    "id": "mmlu-clinicalknowledge",
    "name": "MMLU Clinical Knowledge",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Clinical Knowledge",
    "examples": null,
    "num_of_dataset_prompts": 265,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "eefc8a5f1324e9e7"
  },
  "cbbq-lite-religion-disamb": {
    "id": "cbbq-lite-religion-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "0f37a7779a25baa8"
  },
  "coqa-conversational-qna": {
    "id": "coqa-conversational-qna",
    "name": "coqa_conversational_question_answering",
    "description": "To measure the ability of machines to understand a text passage and answer a series of interconnected questions",
    "examples": null,
    "num_of_dataset_prompts": 10930,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "0bef8281c615065f"
  },
  "singapore-public-housing": {
    "id": "singapore-public-housing",
    "name": "Singapore Transport System",
    "description": "Contain questions about Singapore's public housing system",
    "examples": null,
    "num_of_dataset_prompts": 12,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "edc4b20568a822cf"
  },
  "challenging-toxicity-prompts": {
    "id": "challenging-toxicity-prompts",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 2392,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "ad54e9a5c32030e7"
  },
  "bbq-lite-race-x-ses-ambiguous": {
    "id": "bbq-lite-race-x-ses-ambiguous",
    "name": "BBQ-lite on race-x-ses - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 5580,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "8d9a37c031859246"
  },
  "mlc-cbr-malicious-user": {
    "id": "mlc-cbr-malicious-user",
    "name": "MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives",
    "description": "Responses that enable, encourage, or endorse the creation of weapons of mass destruction.",
    "examples": null,
    "num_of_dataset_prompts": 1570,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "4027084685446ac8"
  },
  "bbq-lite-gender-ambiguous": {
    "id": "bbq-lite-gender-ambiguous",
    "name": "BBQ-lite on gender - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 2836,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "e8c397a23267f3ab"
  },
  "bbq-lite-sexual-orientation-ambiguous": {
    "id": "bbq-lite-sexual-orientation-ambiguous",
    "name": "BBQ-lite on sexual-orientation - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 432,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "4c35c2ffea29da1b"
  },
  "cbbq-lite-age-ambiguous": {
    "id": "cbbq-lite-age-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 7400,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "aec96b3cb7b5575c"
  },
  "mlc-cbr-typical-user": {
    "id": "mlc-cbr-typical-user",
    "name": "MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives",
    "description": "Responses that enable, encourage, or endorse the creation of weapons of mass destruction.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "77c79ae708b2321a"
  },
  "mlc-hat-typical-user": {
    "id": "mlc-hat-typical-user",
    "name": "MLC Baseline - Hate",
    "description": "Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "f19940b1810dc29a"
  },
  "cbbq-lite-nationality-ambiguous": {
    "id": "cbbq-lite-nationality-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 11988,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "93552ad140f282c6"
  },
  "cbbq-lite-physical-appearance-disamb": {
    "id": "cbbq-lite-physical-appearance-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1856,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "8414dfcd44402723"
  },
  "cbbq-lite-region-disamb": {
    "id": "cbbq-lite-region-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 2176,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "532b288b8d269adf"
  },
  "winogrande": {
    "id": "winogrande",
    "name": "Winogrande",
    "description": "This dataset is used for commonsense reasoning, expert-crafted pronoun resolution problems designed to be unsolvable for statistical models.",
    "examples": null,
    "num_of_dataset_prompts": 41665,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/allenai/winogrande",
    "license": "Apache-2.0",
    "hash": "00d0f86376e01ea4"
  },
  "cbbq-lite-religion-ambiguous": {
    "id": "cbbq-lite-religion-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "0f37a7779a25baa8"
  },
  "contextual-parametric-knowledge-conflicts": {
    "id": "contextual-parametric-knowledge-conflicts",
    "name": "Contextual Parametric Knowledge Conflicts",
    "description": "To measure the model/’s ability in making informed predictions",
    "examples": null,
    "num_of_dataset_prompts": 17528,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/contextual_parametric_knowledge_conflicts",
    "license": "",
    "hash": "c37cf84e59f04922"
  },
  "mmlu-anatomy": {
    "id": "mmlu-anatomy",
    "name": "MMLU Anatomy",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Anatomy",
    "examples": null,
    "num_of_dataset_prompts": 135,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "247b383ea1bf391a"
  },
  "singapore-transport-system": {
    "id": "singapore-transport-system",
    "name": "Singapore Transport System",
    "description": "Contain questions about Singapore's transport system",
    "examples": null,
    "num_of_dataset_prompts": 27,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "6b8147eb853af484"
  },
  "truthfulqa-multiple-open-ended": {
    "id": "truthfulqa-multiple-open-ended",
    "name": "truthfulqa",
    "description": "TruthfulQA aims to measure the the truthfulness of a model.",
    "examples": null,
    "num_of_dataset_prompts": 817,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "c10a207658c1dd7e"
  },
  "arc-easy": {
    "id": "arc-easy",
    "name": "ARC-Easy",
    "description": "A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the easy set.",
    "examples": null,
    "num_of_dataset_prompts": 5197,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://allenai.org/data/arc",
    "license": "CC BY-SA",
    "hash": "c152ca7c4d2b94f0"
  },
  "cbbq-lite-ethnicity-disamb": {
    "id": "cbbq-lite-ethnicity-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 980,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "de647afe47c21c20"
  },
  "singapore-iconic-places": {
    "id": "singapore-iconic-places",
    "name": "Iconic Places in Singapore",
    "description": "Contain questions about Singapore's iconic places.",
    "examples": null,
    "num_of_dataset_prompts": 16,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "7041724ec178f585"
  },
  "cbbq-lite-disability-ambiguous": {
    "id": "cbbq-lite-disability-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1588,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "6107a4b6a95aa1c1"
  },
  "cbbq-lite-SES-ambiguous": {
    "id": "cbbq-lite-SES-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 3960,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "48eec40863828bad"
  },
  "mlc-scr-typical-user": {
    "id": "mlc-scr-typical-user",
    "name": "MLC Baseline - Sex-Related Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of sex-related crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "ee019c39e4cf9ca7"
  },
  "analogical-similarity": {
    "id": "analogical-similarity",
    "name": "Analogical Similarity",
    "description": "To measure the model’s ability in discriminating between different degrees of analogical similarity in two given episodes",
    "examples": null,
    "num_of_dataset_prompts": 323,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/analogical_similarity",
    "license": "",
    "hash": "08223ddaa2d2ca59"
  },
  "cbbq-lite-age-disamb": {
    "id": "cbbq-lite-age-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 7400,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "aec96b3cb7b5575c"
  },
  "bbq-lite-race-ethnicity-disamb": {
    "id": "bbq-lite-race-ethnicity-disamb",
    "name": "BBQ-lite on race-ethnicity - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 3440,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c3b7560255084661"
  },
  "gsm8k": {
    "id": "gsm8k",
    "name": "GSM8K",
    "description": "This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations.",
    "examples": null,
    "num_of_dataset_prompts": 8792,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/openai/grade-school-math",
    "license": "MIT License",
    "hash": "951ad5b9d53848b9"
  },
  "mlc-cae-malicious-user": {
    "id": "mlc-cae-malicious-user",
    "name": "MLC Baseline - Child Abuse and Exploitation",
    "description": "Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.",
    "examples": null,
    "num_of_dataset_prompts": 1330,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "24d09b1438675544"
  },
  "bbq-lite-race-x-gender-disamb": {
    "id": "bbq-lite-race-x-gender-disamb",
    "name": "BBQ-lite on race-x-gender - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 7980,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "88c631ed70ee0742"
  },
  "mlc-scr-malicious-user": {
    "id": "mlc-scr-malicious-user",
    "name": "MLC Baseline - Sex-Related Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of sex-related crimes.",
    "examples": null,
    "num_of_dataset_prompts": 1330,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "5cfa7a3e0fd10c45"
  },
  "bbq-lite-disability-status-ambiguous": {
    "id": "bbq-lite-disability-status-ambiguous",
    "name": "BBQ-lite on disability-status - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 778,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "82fcd89a8c4d0e78"
  },
  "mmlu-collegebiology": {
    "id": "mmlu-collegebiology",
    "name": "MMLU College Biology",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Biology",
    "examples": null,
    "num_of_dataset_prompts": 144,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "e0ac3237ce72f9ce"
  },
  "challenging-toxicity-prompts-variation1": {
    "id": "challenging-toxicity-prompts-variation1",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "e64f6f1f1242c66b"
  },
  "bbq-lite-religion-disamb": {
    "id": "bbq-lite-religion-disamb",
    "name": "BBQ-lite on religion - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 600,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "966c6832d9c894fd"
  },
  "mlc-cae-typical-user": {
    "id": "mlc-cae-typical-user",
    "name": "MLC Baseline - Child Abuse and Exploitation",
    "description": "Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "31d9cc15ebd93fb1"
  },
  "cbbq-lite-nationality-disamb": {
    "id": "cbbq-lite-nationality-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 11988,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "93552ad140f282c6"
  },
  "cbbq-lite-region-ambiguous": {
    "id": "cbbq-lite-region-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 2176,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "532b288b8d269adf"
  },
  "cbbq-lite-household-registration-disamb": {
    "id": "cbbq-lite-household-registration-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 8700,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "c6bbdf9915399bee"
  },
  "cbbq-lite-physical-appearance-ambiguous": {
    "id": "cbbq-lite-physical-appearance-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1856,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "8414dfcd44402723"
  },
  "mlc-nvc-typical-user": {
    "id": "mlc-nvc-typical-user",
    "name": "MLC Baseline - Non-Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "5d285c725672c9ed"
  },
  "mlc-nvc-malicious-user": {
    "id": "mlc-nvc-malicious-user",
    "name": "MLC Baseline - Non-Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 1810,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "7ff05c2b42422067"
  },
  "occupation": {
    "id": "occupation",
    "name": "List of Occupations",
    "description": "This dataset contains a list of occupations collected from various websites (e.g., MoM of Singapore). The dataset can be used to test job-related applications.",
    "examples": null,
    "num_of_dataset_prompts": 7270,
    "created_date": "2025-01-07 13:09:09",
    "reference": "IMDA",
    "license": "MIT License",
    "hash": "69403f6bf8954378"
  },
  "jailbreak-dan": {
    "id": "jailbreak-dan",
    "name": "In-the-wild Jailbreak Dataset",
    "description": "This dataset contains jailbreak prompts that were collected from various places (e.g., Discord).",
    "examples": null,
    "num_of_dataset_prompts": 22,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/verazuo/jailbreak_llms",
    "license": "MIT license",
    "hash": "641aaf7640626d6b"
  },
  "mlc-ssh-vulnerable-user": {
    "id": "mlc-ssh-vulnerable-user",
    "name": "MLC Baseline - Suicide & Self-Harm",
    "description": "Responses that enable, encourage, or endorse acts of intentional self-harm.",
    "examples": null,
    "num_of_dataset_prompts": 1090,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "e305117629194941"
  },
  "mlc-vcr-typical-user": {
    "id": "mlc-vcr-typical-user",
    "name": "MLC Baseline - Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "",
    "hash": "803f333c0e727b2f"
  },
  "commonsense-morality-hard-variation1": {
    "id": "commonsense-morality-hard-variation1",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "f736173742bcc35f"
  },
  "auto-categorisation": {
    "id": "auto-categorisation",
    "name": "auto_categorization",
    "description": "To measure the model's ability in categorising elements into groups.",
    "examples": null,
    "num_of_dataset_prompts": 328,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/auto_categorization",
    "license": "",
    "hash": "475eb5ba8102a91c"
  },
  "bbq-lite-sexual-orientation-disamb": {
    "id": "bbq-lite-sexual-orientation-disamb",
    "name": "BBQ-lite on sexual-orientation - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 432,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "61bc89d90e97ec60"
  },
  "tamil-tamilnews-classification": {
    "id": "tamil-tamilnews-classification",
    "name": "tamil-news-classification",
    "description": "This dataset is used to classify a static set of Tamil News. The task is to classify news to its respective category. The dataset has 6 news categories - \"tamilnadu\", \"india\", \"cinema\", \"sports\", \"politics\", \"world\".",
    "examples": null,
    "num_of_dataset_prompts": 3631,
    "created_date": "2025-01-07 13:09:10",
    "reference": "",
    "license": "GNU General Public License v3.0",
    "hash": "13e4831e1ac7cfe8"
  },
  "cbbq-lite-sexual-orientation-disamb": {
    "id": "cbbq-lite-sexual-orientation-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 560,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "66c5c6934a4fcbed"
  },
  "cvalues": {
    "id": "cvalues",
    "name": "C-Values - Values of Chinese Large Language attack_modules",
    "description": "This dataset contains prompts that can measure the alignment ability of LLMs in both safety and responsibility in Chinese Language.",
    "examples": null,
    "num_of_dataset_prompts": 1712,
    "created_date": "2025-01-07 13:09:09",
    "reference": "",
    "license": "Apache-2.0 license",
    "hash": "a8f90de7237ccc48"
  },
  "bbq-lite-physical-appearance-ambiguous": {
    "id": "bbq-lite-physical-appearance-ambiguous",
    "name": "BBQ-lite on physical-appearance - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 788,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "3f6c22ff320de30f"
  },
  "privacy-enronemails": {
    "id": "privacy-enronemails",
    "name": "enronemail",
    "description": "The Enron email dataset is aimed at evaluating if there is leakage of private information. The dataset was used in DecodingTrust as part of its privacy evaluation.",
    "examples": null,
    "num_of_dataset_prompts": 166418,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/ParakweetLabs/EmailIntentDataSet",
    "license": "Apache-2.0",
    "hash": "7976cf2e5a12b845"
  },
  "mmlu-medicalgenetics": {
    "id": "mmlu-medicalgenetics",
    "name": "MMLU Medical Genetics",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Medical Genetics",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "fda8c8823e758ec7"
  },
  "cbbq-lite-gender-ambiguous": {
    "id": "cbbq-lite-gender-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1534,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "ab7009936747ecb3"
  },
  "bbq-lite-ses-ambiguous": {
    "id": "bbq-lite-ses-ambiguous",
    "name": "BBQ-lite on ses - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 3432,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "02115458961a59c0"
  },
  "truthfulqa-open-ended": {
    "id": "truthfulqa-open-ended",
    "name": "truthfulqa",
    "description": "TruthfulQA aims to measure the the truthfulness of a model.",
    "examples": null,
    "num_of_dataset_prompts": 817,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "52543f32ebf9126c"
  },
  "commonsense-morality-easy-variation1": {
    "id": "commonsense-morality-easy-variation1",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "65e3afcb0f8ad651"
  },
  "singapore-places-tnf": {
    "id": "singapore-places-tnf",
    "name": "Places in Singapore",
    "description": "Contain prompts that test model's udnerstanding places in Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "f632d6e401432bd2"
  },
  "bbq-lite-nationality-disamb": {
    "id": "bbq-lite-nationality-disamb",
    "name": "BBQ-lite on nationality - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 1540,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "2af88b62a03be022"
  },
  "mmlu-professionalmedicine": {
    "id": "mmlu-professionalmedicine",
    "name": "MMLU Professional Medicine.json",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Professional Medicine.json",
    "examples": null,
    "num_of_dataset_prompts": 272,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "94f3e264a8908a67"
  },
  "cbbq-lite-disease-disamb": {
    "id": "cbbq-lite-disease-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 586,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "b0c3b61bd4dcab73"
  },
  "bbq-lite-race-ethnicity-ambiguous": {
    "id": "bbq-lite-race-ethnicity-ambiguous",
    "name": "BBQ-lite on race-ethnicity - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 3440,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "220f8bdc78b12952"
  },
  "cbbq-lite-disability-disamb": {
    "id": "cbbq-lite-disability-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1588,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "6107a4b6a95aa1c1"
  },
  "winobias-type1": {
    "id": "winobias-type1",
    "name": "winobias-variation1",
    "description": "This dataset contains gender-bias based on the professions from the Labor Force Statistics (https://www.bls.gov/cps/cpsaat11.htm), which contain some gender-bias.",
    "examples": null,
    "num_of_dataset_prompts": 396,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino",
    "license": "MIT License",
    "hash": "708e89076af6b5a3"
  },
  "cbbq-lite-educational-qualification-ambiguous": {
    "id": "cbbq-lite-educational-qualification-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1392,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "2bb25a9da17b352e"
  },
  "cbbq-lite-race-ambiguous": {
    "id": "cbbq-lite-race-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "0f37a7779a25baa8"
  },
  "hellaswag": {
    "id": "hellaswag",
    "name": "HellaSwag",
    "description": "This dataset is used to evaluate commonsense with questions that are trivial for humans but difficult for state-of-the-art models.",
    "examples": null,
    "num_of_dataset_prompts": 49947,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/rowanz/hellaswag",
    "license": "MIT License",
    "hash": "62c161c4ad177a2f"
  },
  "bbq-lite-ses-disamb": {
    "id": "bbq-lite-ses-disamb",
    "name": "BBQ-lite on ses - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 3432,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "487e15048dc8cf66"
  },
  "singapore-safety-questions": {
    "id": "singapore-safety-questions",
    "name": "Safety Benchmark (Singapore Context)",
    "description": "Contain prompts that test safety in Singapore-context",
    "examples": null,
    "num_of_dataset_prompts": 59,
    "created_date": "2025-01-07 13:09:10",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "ce62c266e73dec45"
  },
  "medqa-us": {
    "id": "medqa-us",
    "name": "MedQA (US)",
    "description": "MedQA is a free-form multiple-choice OpenQA dataset for solving medical problems. These are collected from the professional medical board exams. We extracted the list of MCQ questions from the US in this dataset.",
    "examples": null,
    "num_of_dataset_prompts": 10178,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/jind11/MedQA",
    "license": "MIT License",
    "hash": "4610841dac98f56d"
  },
  "uciadult": {
    "id": "uciadult",
    "name": "uciadult",
    "description": "The UCI adult dataset, created in 1996, is used to train models to predict whether a person's income will exceed $50K/yr based on census data. Also known as \"Census Income\" dataset. ",
    "examples": null,
    "num_of_dataset_prompts": 32561,
    "created_date": "2025-01-07 13:09:10",
    "reference": "https://archive.ics.uci.edu/dataset/2/adult",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "339cd799ea0bc927"
  },
  "mmlu-all": {
    "id": "mmlu-all",
    "name": "MMLU",
    "description": "This dataset covers 57 tasks including elementary mathemathics, US history, computer science, law, and more.",
    "examples": null,
    "num_of_dataset_prompts": 17487,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT license",
    "hash": "ecadaffdfb18afa7"
  },
  "cbbq-lite-sexual-orientation-ambiguous": {
    "id": "cbbq-lite-sexual-orientation-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 560,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "66c5c6934a4fcbed"
  },
  "cbbq-lite-ethnicity-ambiguous": {
    "id": "cbbq-lite-ethnicity-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 980,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "de647afe47c21c20"
  },
  "bbq-lite-disability-status-disamb": {
    "id": "bbq-lite-disability-status-disamb",
    "name": "BBQ-lite on disability-status - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 778,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "b0078eddcdb186ff"
  },
  "m3exam-vietnamese-test": {
    "id": "m3exam-vietnamese-test",
    "name": "m3exam-vietnamese",
    "description": "This dataset contains local exam questions for testing Vietnamese capability.",
    "examples": null,
    "num_of_dataset_prompts": 1745,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "f98f818e6651b5e8"
  },
  "m3exam-indonesian-test": {
    "id": "m3exam-indonesian-test",
    "name": "m3exam-indonesian",
    "description": "This dataset contains local exam questions for testing Indonesian capability.",
    "examples": null,
    "num_of_dataset_prompts": 1580,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "c6592b38217505be"
  },
  "m3exam-thai-test": {
    "id": "m3exam-thai-test",
    "name": "m3exam-thai",
    "description": "This dataset contains local exam questions for testing Thai capability.",
    "examples": null,
    "num_of_dataset_prompts": 2126,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "30cb08abf98b36ed"
  },
  "clcc": {
    "id": "clcc",
    "name": "Chinese Linguistics & Cognition Challenge (CLCC)",
    "description": "This dataset is a subjective benchmark created by BAAI FlagEval group.",
    "examples": null,
    "num_of_dataset_prompts": 760,
    "created_date": "2025-01-07 13:09:08",
    "reference": "https://flageval.baai.ac.cn/",
    "license": "CC-BY-4.0 license",
    "hash": "d2c54aa79e09485c"
  },
  "i2p-text2image-prompts": {
    "id": "i2p-text2image-prompts",
    "name": "Inappropriate Image Prompts (I2P)",
    "description": "The I2P benchmark contains real user prompts for generative text2image prompts that are unproportionately likely to produce inappropriate images. I2P was introduced in the 2023 CVPR paper Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. This benchmark is not specific to any approach or model, but was designed to evaluate mitigating measures against inappropriate degeneration in Stable Diffusion.",
    "examples": null,
    "num_of_dataset_prompts": 4703,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://huggingface.co/datasets/AIML-TUDA/i2p",
    "license": "MIT License",
    "hash": "83886043cf8d7117"
  },
  "gender-occupation-text2-image-prompts": {
    "id": "gender-occupation-text2-image-prompts",
    "name": "Gender Occupational Bias",
    "description": "The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759",
    "examples": null,
    "num_of_dataset_prompts": 13,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://arxiv.org/abs/2211.03759",
    "license": "MIT License",
    "hash": "e181f8d5b15736b9"
  },
  "gender-text2-image-prompts": {
    "id": "gender-text2-image-prompts",
    "name": "Gender Occupational Bias",
    "description": "The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759",
    "examples": null,
    "num_of_dataset_prompts": 13,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://arxiv.org/abs/2211.03759",
    "license": "MIT License",
    "hash": "e181f8d5b15736b9"
  },
  "cyberseceval_promptinjection": {
    "id": "cyberseceval_promptinjection",
    "name": "CyberSecEval Prompt Injection",
    "description": "Check if llm is susceptible to prompt injection",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-07 13:09:09",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "fa0d855677c5774a"
  },
  "facticity-apac-multilungual-facts": {
    "id": "facticity-apac-multilungual-facts",
    "name": "Facts about Asia pacific in True and False in 4 languages (Chinese, Malay, Tamil and English)",
    "description": "Contain prompts that contain facts about Asia Pacific and Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 22,
    "created_date": "2025-01-07 13:09:09",
    "reference": "facticity.ai",
    "license": "Apache-2.0",
    "hash": "05f9b769b8b26901"
  },
  "squad-v2": {
    "id": "squad-v2",
    "name": "squad-v2",
    "description": "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.",
    "examples": null,
    "num_of_dataset_prompts": 11873,
    "created_date": "2025-01-07 13:10:48",
    "reference": "rajpurkar/squad_v2 · Datasets at Hugging Face",
    "license": "CC BY-SA 4.0",
    "hash": "8efe2a281c4d4334"
  },
  "gsm8k-test": {
    "id": "gsm8k-test",
    "name": "GSM8K-TEST",
    "description": "This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations. The dataset includes only the test set of GSM8K.",
    "examples": null,
    "num_of_dataset_prompts": 1319,
    "created_date": "2025-01-07 13:10:48",
    "reference": "https://github.com/openai/grade-school-math",
    "license": "MIT License",
    "hash": "70380588a44e3d0a"
  },
  "answercarefully-kr": {
    "id": "answercarefully-kr",
    "name": "Answercarefully Information Korean",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-07 13:10:48",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "c338b8f5ceb459be"
  },
  "cyberseceval-promptinjection2-en": {
    "id": "cyberseceval-promptinjection2-en",
    "name": "CyberSecEval Prompt Injection 2 English",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-07 13:10:48",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "13d539e95c06bec6"
  },
  "mlc-ipv-en": {
    "id": "mlc-ipv-en",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - English",
    "description": "Test whether responses violate the intellectual property rights of any party.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:10:48",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "b7d1272250061066"
  },
  "answercarefully-ch": {
    "id": "answercarefully-ch",
    "name": "Answercarefully Information Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-07 10:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "e252ee595a66daa7"
  },
  "answercarefully-en": {
    "id": "answercarefully-en",
    "name": "Answercarefully Information English",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 13:10:48",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "314843a5269726b6"
  },
  "answercarefully-jp": {
    "id": "answercarefully-jp",
    "name": "Answercarefully Information Japanese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-07 13:10:48",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "6072d84e60c4f5c6"
  },
  "mlc-ncr-en": {
    "id": "mlc-ncr-en",
    "name": "ML Commons Non - Voilent Crimes - English",
    "description": "Test whether responses enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:10:48",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "94bf47b0c04e7aeb"
  },
  "mlc-prv-en": {
    "id": "mlc-prv-en",
    "name": "ML Commons  Ailuminte - Privacy - English",
    "description": "Test whether responses contain sensitive, nonpublic personal information that could undermine someone’s physical, digital, or financial security.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:10:48",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "baff1dd07b2712f6"
  },
  "mlc-vcr-en": {
    "id": "mlc-vcr-en",
    "name": "ML Commons Ailuminte - Violent Crimes - English",
    "description": "Test whether responses enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:10:48",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "300e6322a2afa1b7"
  },
  "answercarefully-ca": {
    "id": "answercarefully-ca",
    "name": "Answercarefully Information Cantonese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 13:38:29",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "221fe89d2514b161"
  },
  "answercarefully-cn": {
    "id": "answercarefully-cn",
    "name": "Answercarefully Information Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-07 13:38:29",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "270c1ae4273f3374"
  },
  "answercarefully-fa": {
    "id": "answercarefully-fa",
    "name": "Answercarefully Information Farsi",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 13:38:29",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "e3b70cefee39fc44"
  },
  "answercarefully-fr": {
    "id": "answercarefully-fr",
    "name": "Answercarefully Information French",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 13:38:29",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "5a9b65fe4d461568"
  },
  "answercarefully-kw": {
    "id": "answercarefully-kw",
    "name": "Answercarefully Information Kwashili",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 13:38:29",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "319a5a469cdd47ff"
  },
  "answercarefully-my": {
    "id": "answercarefully-my",
    "name": "Answercarefully Information Malay",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 13:38:29",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "668a2a9ffbf9a35b"
  },
  "answercarefully-tl": {
    "id": "answercarefully-tl",
    "name": "Answercarefully Information Telugu",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 13:38:29",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "e50c8a99bf9bf331"
  },
  "cyberseceval-promptinjection2-jp": {
    "id": "cyberseceval-promptinjection2-jp",
    "name": "CyberSecEval Prompt Injection 2 Japanese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-07 13:39:35",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "348356366d90eb87"
  },
  "cyberseceval-promptinjection2-kr": {
    "id": "cyberseceval-promptinjection2-kr",
    "name": "CyberSecEval Prompt Injection 2 Korean",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-07 13:39:35",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "3a20d7d42c8a4a80"
  },
  "cyberseceval-promptinjection2-kw": {
    "id": "cyberseceval-promptinjection2-kw",
    "name": "CyberSecEval Prompt Injection 2 Kwashili",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-07 13:39:35",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "f6cc86671b3c322d"
  },
  "mlc-ipv-ca": {
    "id": "mlc-ipv-ca",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Cantonese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "02374af794f91b30"
  },
  "mlc-ipv-cn": {
    "id": "mlc-ipv-cn",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "e92d09996b900ab1"
  },
  "mlc-ipv-fa": {
    "id": "mlc-ipv-fa",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Farsi",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "15f2827cf52970e9"
  },
  "mlc-ipv-fr": {
    "id": "mlc-ipv-fr",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - French",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "a2fcd85eccea6bfb"
  },
  "mlc-ipv-jp": {
    "id": "mlc-ipv-jp",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Japanese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "17e881c778bad919"
  },
  "mlc-ipv-kr": {
    "id": "mlc-ipv-kr",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Korean",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "a90036d4e769d0aa"
  },
  "mlc-ipv-kw": {
    "id": "mlc-ipv-kw",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Kwashili",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "08b7d9e0ecf3befe"
  },
  "mlc-ipv-my": {
    "id": "mlc-ipv-my",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Malay",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "371c8840bcf25301"
  },
  "mlc-ipv-tl": {
    "id": "mlc-ipv-tl",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Telugu",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "7803bb03fde5d8db"
  },
  "mlc-ncr-cn": {
    "id": "mlc-ncr-cn",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "e95d4f5c91cf9e03"
  },
  "mlc-ncr-fa": {
    "id": "mlc-ncr-fa",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Farsi",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "e1fd6ca3166530ad"
  },
  "mlc-ncr-fr": {
    "id": "mlc-ncr-fr",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - French",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "fd3dc97bb2254e8f"
  },
  "mlc-ncr-jp": {
    "id": "mlc-ncr-jp",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Japanese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "1ceefe9a27a3e490"
  },
  "mlc-ncr-kr": {
    "id": "mlc-ncr-kr",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Korean",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "5a915af6306dd262"
  },
  "mlc-ncr-kw": {
    "id": "mlc-ncr-kw",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Kwashili",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "8a7c050eb3fef12f"
  },
  "mlc-ncr-my": {
    "id": "mlc-ncr-my",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Malay",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "e35dd7d299122c6e"
  },
  "mlc-ncr-tl": {
    "id": "mlc-ncr-tl",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Telugu",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "29c7a2b4458c9a6e"
  },
  "mlc-prv-cn": {
    "id": "mlc-prv-cn",
    "name": "ML Commons Ailuminte - Privacy - Chinese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "7852e3095af2006f"
  },
  "mlc-prv-fr": {
    "id": "mlc-prv-fr",
    "name": "ML Commons Ailuminte - Privacy - Farsi",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "81a14f247ce3c70d"
  },
  "mlc-prv-jp": {
    "id": "mlc-prv-jp",
    "name": "ML Commons Ailuminte - Privacy - Japanese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:15",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "0dcc685983008b5e"
  },
  "mlc-prv-kr": {
    "id": "mlc-prv-kr",
    "name": "ML Commons Ailuminte - Privacy - Korean",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "95ada913a35e1c90"
  },
  "mlc-prv-kw": {
    "id": "mlc-prv-kw",
    "name": "ML Commons Ailuminte - Privacy - Kwashili",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "b80862b18b60f578"
  },
  "mlc-prv-my": {
    "id": "mlc-prv-my",
    "name": "ML Commons Ailuminte - Privacy - Malay",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "4edebb5fe7ce1b65"
  },
  "mlc-prv-tl": {
    "id": "mlc-prv-tl",
    "name": "ML Commons Ailuminte - Privacy - Telugu",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "52cf911d089500be"
  },
  "mlc-vcr-cn": {
    "id": "mlc-vcr-cn",
    "name": "ML Commons Ailuminte - Violent Crimes - Chinese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "f0a5a639d99cb6fc"
  },
  "mlc-vcr-fr": {
    "id": "mlc-vcr-fr",
    "name": "ML Commons Ailuminte - Violent Crimes - French",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-04 20:15:48",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "225f809fa74cb11d"
  },
  "mlc-vcr-jp": {
    "id": "mlc-vcr-jp",
    "name": "ML Commons Ailuminte - Violent Crimes - Japanese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "4627b1c6a462f1a3"
  },
  "mlc-vcr-kr": {
    "id": "mlc-vcr-kr",
    "name": "ML Commons Ailuminte - Violent Crimes - Korean",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "9dae89da7b1d49b3"
  },
  "mlc-vcr-kw": {
    "id": "mlc-vcr-kw",
    "name": "ML Commons Ailuminte - Violent Crimes - Kwashili",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "3f848c53ce2300ae"
  },
  "mlc-vcr-my": {
    "id": "mlc-vcr-my",
    "name": "ML Commons Ailuminte - Violent Crimes - Malay",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "d99e02beef9fb648"
  },
  "mlc-vcr-tl": {
    "id": "mlc-vcr-tl",
    "name": "ML Commons Ailuminte - Violent Crimes - Telugu",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-01-07 13:51:16",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0",
    "hash": "8d89966222483b5e"
  }
}