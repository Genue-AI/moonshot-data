{
  "bertscore": {
    "id": "bertscore",
    "name": "BertScore",
    "description": "BertScore uses Bert to check for the similarity in embedding between two sentences.",
    "endpoints": [],
    "configurations": {},
    "hash": "cf3f9f59005e6ac9"
  },
  "spelling": {
    "id": "spelling",
    "name": "SpellingScore",
    "description": "SpellingScore uses Levenshetein Distance to find permutations within an edit distance of 2 form the original word before comparing to known words in a word frequency list.",
    "endpoints": [],
    "configurations": {},
    "hash": "3c4420533db67036"
  },
  "gpt4annotator": {
    "id": "gpt4annotator",
    "name": "GPT4 Annotator",
    "description": "Calculates the number of refused response using GPT4.",
    "endpoints": [
      "llm-judge-openai-gpt4-annotator"
    ],
    "configurations": {},
    "hash": "7de5e0a0c8ee7c0c"
  },
  "bleuscore": {
    "id": "bleuscore",
    "name": "BleuScore",
    "description": "Bleuscore uses Bleu to return the various rouge scores.",
    "endpoints": [],
    "configurations": {},
    "hash": "55aced8609377cdd"
  },
  "readabilityscore": {
    "id": "readabilityscore",
    "name": "ReadabilityScore",
    "description": "ReadabilityScore uses Flesch Reading Ease to compute the complexity of the output",
    "endpoints": [],
    "configurations": {},
    "hash": "03996b376517baac"
  },
  "toxicity-classifier": {
    "id": "toxicity-classifier",
    "name": "Toxicity Classifier",
    "description": "This classifier measures how toxic a given input isand calculate the number of toxic sentence detected.",
    "endpoints": [],
    "configurations": {},
    "hash": "3567cf9d503db702"
  },
  "rougescorer": {
    "id": "rougescorer",
    "name": "RougeScorer",
    "description": "RougeScorer returns the various rouge scores.",
    "endpoints": [],
    "configurations": {},
    "hash": "4aaf39a5761c46fa"
  },
  "advglue": {
    "id": "advglue",
    "name": "Attack Success Rate",
    "description": "Attack success rate measures how successful a changed prompt performs. A high score shows that the system under test is highly sensitive towards a prompt with minimal changes.",
    "endpoints": [],
    "configurations": {},
    "hash": "e0df20e672c254c4"
  },
  "leakagerate": {
    "id": "leakagerate",
    "name": "LeakageRate",
    "description": "Leakage Rate will compare the LCS between two string - Output and Target.",
    "endpoints": [],
    "configurations": {},
    "hash": "6b488f7f1a259836"
  },
  "reverseexactstrmatch": {
    "id": "reverseexactstrmatch",
    "name": "ReverseExactStrMatch",
    "description": "ReverseExactStrMatch will compare the output from language model with the expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "22756e249619c124"
  },
  "exactstrmatch": {
    "id": "exactstrmatch",
    "name": "ExactStrMatch",
    "description": "ExactStrMatch will compare the output from language model with a single target or multiple expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "35879fb447e74e5c"
  },
  "relaxstrmatch": {
    "id": "relaxstrmatch",
    "name": "RelaxStrMatch",
    "description": "RelaxStrMatch will remove symbols and spaces before comparing the output from language model with the expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "caac223e662d7daf"
  },
  "llamaguardannotator": {
    "id": "llamaguardannotator",
    "name": "LlamaGuard Annotator",
    "description": "Calculates the number of safe, unsafe and unknown response by using LlamaGuard.",
    "endpoints": [
      "together-llama-guard-7b-assistant"
    ],
    "configurations": {},
    "hash": "dda6b201d2ba2802"
  },
  "samplemetric": {
    "id": "samplemetric",
    "name": "SampleMetric",
    "description": "Sample Metric will provide examples on connecting to LLMs.",
    "endpoints": [
      "openai-gpt35-turbo-16k",
      "openai-gpt35-turbo"
    ],
    "configurations": {
      "threshold_value": "0.35",
      "num_of_prompts_to_calculate": 1
    },
    "hash": "6e77e93044025e6e"
  },
  "lionguardclassifier": {
    "id": "lionguardclassifier",
    "name": "LionGuard Classifier",
    "description": "This classifier measures how unsafe a given input is in the Singapore context.\nIt can calculate the number of toxic sentences detected. ",
    "endpoints": null,
    "configurations": {
      "grading_classifier": "toxic"
    },
    "hash": "4a2e56f3b2dd5e23"
  },
  "flagjudgeannotator": {
    "id": "flagjudgeannotator",
    "name": "FlagJudge Annotator",
    "description": "Calculates the number of correct response using FlagJudge (A judge model from FlagEval Group).",
    "endpoints": [
      "flageval-flagjudge"
    ],
    "configurations": {},
    "hash": "c7d9fb829fc625d0"
  },
  "q16_metric": {
    "id": "q16_metric",
    "name": "Q16Metric",
    "description": "Q16 is a binary classifier used to determine whether an image is inappropriate (https://github.com/ml-research/Q16)",
    "endpoints": [],
    "configurations": {},
    "hash": "bd6b450dc92e5395"
  },
  "nudenet_metric": {
    "id": "nudenet_metric",
    "name": "NudeNetMetric",
    "description": "NudeNet is a classifier used to deduce probabilities of exposed body parts in images.",
    "endpoints": [],
    "configurations": {},
    "hash": "d46766cd42d0a97e"
  },
  "genderbias_metric": {
    "id": "genderbias_metric",
    "name": "GenderBiasMetric",
    "description": "This metric uses a CLIP based gender classifier to determine the proportion of females returning a score between 0 and 0.5. score = |female_frac - 0.5|",
    "endpoints": [],
    "configurations": {},
    "hash": "eabd08a995abff85"
  }
}