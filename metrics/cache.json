{
  "bertscore": {
    "id": "bertscore",
    "name": "BertScore",
    "description": "BertScore uses Bert to check for the similarity in embedding between two sentences.",
    "endpoints": [],
    "configurations": {},
    "hash": "ee42112af9752856"
  },
  "spelling": {
    "id": "spelling",
    "name": "SpellingScore",
    "description": "SpellingScore uses Levenshetein Distance to find permutations within an edit distance of 2 form the original word before comparing to known words in a word frequency list.",
    "endpoints": [],
    "configurations": {},
    "hash": "3c38e21f9cae07dd"
  },
  "gpt4annotator": {
    "id": "gpt4annotator",
    "name": "GPT4 Annotator",
    "description": "Calculates the number of refused response using GPT4.",
    "endpoints": [
      "llm-judge-openai-gpt4-annotator"
    ],
    "configurations": {},
    "hash": "ac7d105a2e54c4c2"
  },
  "bleuscore": {
    "id": "bleuscore",
    "name": "BleuScore",
    "description": "Bleuscore uses Bleu to return the various rouge scores.",
    "endpoints": [],
    "configurations": {},
    "hash": "1a5576018c05c74c"
  },
  "readabilityscore": {
    "id": "readabilityscore",
    "name": "ReadabilityScore",
    "description": "ReadabilityScore uses Flesch Reading Ease to compute the complexity of the output",
    "endpoints": [],
    "configurations": {},
    "hash": "a7e42d63c19b8da1"
  },
  "toxicity-classifier": {
    "id": "toxicity-classifier",
    "name": "Toxicity Classifier",
    "description": "This classifier measures how toxic a given input isand calculate the number of toxic sentence detected.",
    "endpoints": [],
    "configurations": {},
    "hash": "34aab8b3bbd51c4a"
  },
  "rougescorer": {
    "id": "rougescorer",
    "name": "RougeScorer",
    "description": "RougeScorer returns the various rouge scores.",
    "endpoints": [],
    "configurations": {},
    "hash": "31d39657bbc8275e"
  },
  "advglue": {
    "id": "advglue",
    "name": "Attack Success Rate",
    "description": "Attack success rate measures how successful a changed prompt performs. A high score shows that the system under test is highly sensitive towards a prompt with minimal changes.",
    "endpoints": [],
    "configurations": {},
    "hash": "a6ea10487b08b8fe"
  },
  "leakagerate": {
    "id": "leakagerate",
    "name": "LeakageRate",
    "description": "Leakage Rate will compare the LCS between two string - Output and Target.",
    "endpoints": [],
    "configurations": {},
    "hash": "a1a7260849dc3232"
  },
  "reverseexactstrmatch": {
    "id": "reverseexactstrmatch",
    "name": "ReverseExactStrMatch",
    "description": "ReverseExactStrMatch will compare the output from language model with the expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "f4c7b9ad9919a7df"
  },
  "exactstrmatch": {
    "id": "exactstrmatch",
    "name": "ExactStrMatch",
    "description": "ExactStrMatch will compare the output from language model with a single target or multiple expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "37240281bd8af06b"
  },
  "relaxstrmatch": {
    "id": "relaxstrmatch",
    "name": "RelaxStrMatch",
    "description": "RelaxStrMatch will remove symbols and spaces before comparing the output from language model with the expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "4a8fd6f19c2aa58d"
  },
  "llamaguardannotator": {
    "id": "llamaguardannotator",
    "name": "LlamaGuard Annotator",
    "description": "Calculates the number of safe, unsafe and unknown response by using LlamaGuard.",
    "endpoints": [
      "together-llama-guard-7b-assistant"
    ],
    "configurations": {},
    "hash": "c3f0532ceb37cf3b"
  },
  "samplemetric": {
    "id": "samplemetric",
    "name": "SampleMetric",
    "description": "Sample Metric will provide examples on connecting to LLMs.",
    "endpoints": [
      "openai-gpt35-turbo-16k",
      "openai-gpt35-turbo"
    ],
    "configurations": {
      "threshold_value": "0.35",
      "num_of_prompts_to_calculate": 1
    },
    "hash": "0d75c80a1d35bee1"
  },
  "lionguardclassifier": {
    "id": "lionguardclassifier",
    "name": "LionGuard Classifier",
    "description": "This classifier measures how unsafe a given input is in the Singapore context.\nIt can calculate the number of toxic sentences detected. ",
    "endpoints": [],
    "configurations": {
      "grading_classifier": "toxic"
    },
    "hash": "27bf439d34f08a5e"
  },
  "flagjudgeannotator": {
    "id": "flagjudgeannotator",
    "name": "FlagJudge Annotator",
    "description": "Calculates the number of correct response using FlagJudge (A judge model from FlagEval Group).",
    "endpoints": [
      "flageval-flagjudge"
    ],
    "configurations": {},
    "hash": "38b63d7a54da10bd"
  },
  "q16_metric": {
    "id": "q16_metric",
    "name": "Q16Metric",
    "description": "Q16 is a binary classifier used to determine whether an image is inappropriate (https://github.com/ml-research/Q16)",
    "endpoints": [],
    "configurations": {},
    "hash": "5a54a1bd4b1b2b1b"
  },
  "nudenet_metric": {
    "id": "nudenet_metric",
    "name": "NudeNetMetric",
    "description": "NudeNet is a classifier used to deduce probabilities of exposed body parts in images.",
    "endpoints": [],
    "configurations": {},
    "hash": "b0fc76a126ae0981"
  },
  "genderbias_metric": {
    "id": "genderbias_metric",
    "name": "GenderBiasMetric",
    "description": "This metric uses a CLIP based gender classifier to determine the proportion of females returning a score between 0 and 0.5. score = |female_frac - 0.5|",
    "endpoints": [],
    "configurations": {},
    "hash": "3f89eb9e06744522"
  },
  "cybersecevalannotator": {
    "id": "cybersecevalannotator",
    "name": "General Judge LLM",
    "description": "Calculates number of yes and no replied by judge llm  (yes means prompt injection succeed)",
    "endpoints": [
      "llm-judge-azure-gpt4-annotator"
    ],
    "configurations": {},
    "hash": "a55ddb4fa33c5c89"
  }
}